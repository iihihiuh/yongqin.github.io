---
title: "MPC-Pipe: An Efficient Pipeline Scheme for Semi-honest MPC Machine Learning"
collection: publications
permalink: /publications/2023-04-17-mpc-pipe
excerpt: 
date: 2023-06-19
venue: '2024 ACM 29th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)'
paperurl: 'https://arxiv.org/abs/2209.13643'
citation: 'Yongqin Wang, Rachit Rajat, Murali Annavaram, "MPC-Pipe: An Efficient Pipeline Scheme for Semi-honest MPC Machine Learning," 2024 ACM 29th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'24).'
---
Data confidentiality is becoming a significant concern, especially in the cloud computing era. Memory access patterns have been demonstrated to leak critical information such as security keys and a program's spatial and temporal information. This information leak poses an even more significant privacy challenge in machine learning models with embedding tables. Embedding tables are routinely used to learn categorical features from training data. Even knowing the locations of the embedding table entries accessed, not the data within the embedding table, will compromise categorical input data to the model. Embedding entries are privacy-sensitive since they disclose valuable properties about the user. Oblivious RAM (ORAM), and its enhanced variants such as PathORAM have emerged as viable solutions to hide leakage from memory access streams.
In this work, we present LAORAM, an ORAM framework explicitly designed to protect user privacy during embedding table training. LAORAM exploits the unique property of training, the training samples used in the future are known beforehand. LAORAM preprocesses the training samples to identify the memory blocks which are accessed together in the near future. The system tries to assign these blocks to as few paths as possible within the PathORAM infrastructure.
LAORAM does this operation by combining multiple blocks accessed together as superblocks. To further increase performance, LAORAM uses a fat-tree structure for PathORAM reducing the number of background evictions required, which improves the stash usage. We have evaluated LAORAM using both a recommendation model (DLRM) and a NLP model (XLM-R) embedding table configurations. LAORAM performs 5 times faster than PathORAM on a recommendation dataset (Kaggle) and 5.4x faster on a NLP dataset (XNLI), while guaranteeing the same security guarantees as the original PathORAM.